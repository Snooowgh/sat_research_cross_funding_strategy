# coding=utf-8
"""
@Project     : darwin_light
@Author      : Arson
@File Name   : simple_arbitrage_manager_fixed
@Description : å¤šè¿›ç¨‹å¥—åˆ©ç®¡ç†å™¨ - è´Ÿè´£å¯åŠ¨å¼•æ“å’Œé£æ§ç¼“å­˜åˆ†å‘ï¼ˆå¤šè¿›ç¨‹ç‰ˆæœ¬ï¼‰
@Time        : 2025/10/9
"""
import time
import multiprocessing as mp
from typing import Dict, List, Optional
from dataclasses import dataclass
import signal
from arbitrage_param import MultiExchangeArbitrageParam, ArbitrageWhiteListParam
from multi_exchange_info_show import get_multi_exchange_info_combined_model
from cex_tools.exchange_model.multi_exchange_info_model import MultiExchangeCombinedInfoModel
from utils.notify_tools import async_notify_telegram, CHANNEL_TYPE
from cex_tools.exchange_ws.stream_factory import StreamFactory
import asyncio
from loguru import logger
from logic.realtime_hedge_engine import RealtimeHedgeEngine, TradeConfig, RiskConfig


@dataclass
class EngineConfig:
    """å¼•æ“é…ç½®"""
    symbol: str
    exchange1_code: str
    exchange2_code: str
    daemon_mode: bool = True


@dataclass
class EngineHealthMetrics:
    """å¼•æ“å¥åº·æŒ‡æ ‡"""
    process_id: int
    start_time: float
    restart_count: int = 0
    last_error: Optional[str] = None
    consecutive_failures: int = 0
    is_healthy: bool = True
    last_trade_time: Optional[float] = None
    memory_usage_mb: float = 0.0

@dataclass
class ManagerConfig:
    """ç®¡ç†å™¨é…ç½®"""
    # æ›´æ–°é—´éš”
    risk_update_interval_min: int = 2  # é£æ§æ•°æ®æ›´æ–°é—´éš”(åˆ†é’Ÿ)
    engine_check_interval_min: int = 15  # å¼•æ“çŠ¶æ€æ£€æŸ¥é—´éš”(åˆ†é’Ÿ)

    # é€šçŸ¥é…ç½®
    enable_notifications: bool = True
    notify_interval_min: int = 30  # é€šçŸ¥é—´éš”(åˆ†é’Ÿ)

    # å¯åŠ¨é…ç½®
    engine_startup_delay_sec: float = 5.0  # å¼•æ“å¯åŠ¨é—´éš”(ç§’)ï¼Œé¿å…APIè¯·æ±‚è¿‡å¤š

    # å¥åº·ç®¡ç†é…ç½®
    max_restart_attempts: int = 3  # æœ€å¤§é‡å¯å°è¯•æ¬¡æ•°
    restart_backoff_factor: float = 2.0  # é‡å¯é€€é¿å› å­
    memory_limit_mb: float = 1000.0  # å†…å­˜é™åˆ¶(MB)
    no_trade_timeout_min: int = 30  # æ— äº¤æ˜“è¶…æ—¶æ—¶é—´(åˆ†é’Ÿ)


async def create_stream_for_exchange(exchange_code: str, symbol: str):
    """ä¸ºæŒ‡å®šäº¤æ˜“æ‰€åˆ›å»ºWebSocketæµ - ä½¿ç”¨ç°æœ‰å·¥å‚ç±»"""
    try:
        # è½¬æ¢symbolæ ¼å¼ï¼šBTC -> BTCUSDT
        if not symbol.endswith('USDT'):
            full_symbol = f"{symbol}USDT"
        else:
            full_symbol = symbol

        stream = await StreamFactory.create_orderbook_stream(exchange_code, full_symbol)
        return stream

    except Exception as e:
        logger.error(f"âŒ åˆ›å»º {exchange_code} {symbol} WebSocketæµå¤±è´¥: {e}")
        return None

def run_real_engine_in_process(engine_config: EngineConfig,
                               risk_data_dict: Dict, stop_event):
    """
    åœ¨ç‹¬ç«‹è¿›ç¨‹ä¸­è¿è¡ŒçœŸæ­£çš„äº¤æ˜“å¼•æ“

    Args:
        engine_config: å¼•æ“é…ç½®
        risk_data_dict: å…±äº«çš„é£æ§æ•°æ®å­—å…¸
        stop_event: åœæ­¢äº‹ä»¶
    """
    async def engine_main():
        """çœŸæ­£çš„äº¤æ˜“å¼•æ“ä¸»ç¨‹åº"""
        engine = None
        try:
            # è®¾ç½®è¿›ç¨‹æ—¥å¿—
            logger.add(
                f"logs/engine_{engine_config.symbol}_{engine_config.exchange1_code}_{engine_config.exchange2_code}.log",
                rotation="1 day",
                retention="7 days",
                level="INFO",
                format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {message}"
            )

            logger.info(f"ğŸš€ å¯åŠ¨ {engine_config.symbol}({engine_config.exchange1_code}-{engine_config.exchange2_code}) äº¤æ˜“å¼•æ“ ")

            # åˆå§‹åŒ–äº¤æ˜“æ‰€å‚æ•°
            arbitrage_param = MultiExchangeArbitrageParam(auto_init=True)
            await arbitrage_param.init_async_exchanges()
            await asyncio.sleep(0.3)  # ç¡®ä¿äº¤æ˜“æ‰€åˆå§‹åŒ–å®Œæˆ
            # è·å–äº¤æ˜“æ‰€å®ä¾‹
            exchange1 = arbitrage_param.async_exchanges[engine_config.exchange1_code]
            exchange2 = arbitrage_param.async_exchanges[engine_config.exchange2_code]

            # åˆ›å»ºäº¤æ˜“é…ç½®
            trade_config = TradeConfig(
                pair1=f"{engine_config.symbol}USDT",
                pair2=f"{engine_config.symbol}USDT",
                side1="BUY",  # daemonæ¨¡å¼ä¸‹ç”±å¼•æ“è‡ªåŠ¨å†³å®š
                side2="SELL",
                daemon_mode=True,
                no_trade_timeout_sec=0  # æŒç»­è¿è¡Œ
            )

            # åˆ›å»ºé£æ§é…ç½®
            risk_config = RiskConfig()

            # åˆ›å»ºWebSocketæµ
            stream1 = await create_stream_for_exchange(engine_config.exchange1_code, engine_config.symbol)
            stream2 = await create_stream_for_exchange(engine_config.exchange2_code, engine_config.symbol)

            if stream1 is None or stream2 is None:
                logger.error(f"âŒ {engine_config.symbol} WebSocketæµåˆ›å»ºå¤±è´¥ï¼Œæ— æ³•å¯åŠ¨äº¤æ˜“å¼•æ“")
                raise RuntimeError("WebSocketæµåˆ›å»ºå¤±è´¥ï¼Œæ— æ³•å¯åŠ¨äº¤æ˜“å¼•æ“")

            engine = RealtimeHedgeEngine(
                stream1=stream1,
                stream2=stream2,
                exchange1=exchange1,
                exchange2=exchange2,
                trade_config=trade_config,
                risk_config=risk_config,
                exchange_combined_info_cache=risk_data_dict
            )

            # å¯åŠ¨å¼•æ“
            await engine.start()

            # è¿è¡Œç›´åˆ°æ”¶åˆ°åœæ­¢ä¿¡å·
            while not stop_event.is_set():
                # ä½¿ç”¨æ›´çŸ­çš„ç¡çœ ä»¥ä¾¿æ›´å¿«å“åº”åœæ­¢ä¿¡å·
                try:
                    await asyncio.wait_for(asyncio.sleep(0.5), timeout=1.0)
                except asyncio.TimeoutError:
                    continue

                # å®šæœŸæ›´æ–°é£æ§æ•°æ®
                if 'risk_data' in risk_data_dict:
                    engine.exchange_combined_info_cache = risk_data_dict['risk_data']

            logger.info(f"ğŸ›‘ {engine_config.symbol} å¼•æ“æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œæ­£åœ¨åœæ­¢...")
            # åœæ­¢å¼•æ“ï¼ˆæ·»åŠ è¶…æ—¶æ§åˆ¶ï¼‰
            try:
                await asyncio.wait_for(engine.stop(), timeout=3.0)
            except asyncio.TimeoutError:
                logger.warning(f"âš ï¸ {engine_config.symbol} å¼•æ“åœæ­¢è¶…æ—¶ï¼Œå¼ºåˆ¶ç»§ç»­")

            logger.info(f"ğŸ›‘ {engine_config.symbol} å¼•æ“æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œæ­£åœ¨å…³é—­...")

        except Exception as e:
            logger.error(f"âŒ {engine_config.symbol} å¼•æ“è¿›ç¨‹å¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()
        finally:
            try:
                if engine is not None:
                    await engine.stop()
                logger.info(f"âœ… {engine_config.symbol} å¼•æ“å·²å…³é—­")
            except Exception as e:
                logger.error(f"âŒ å…³é—­ {engine_config.symbol} å¼•æ“å¤±è´¥: {e}")

    # è¿è¡Œå¼•æ“ä¸»ç¨‹åº
    try:
        asyncio.run(engine_main())
    except KeyboardInterrupt:
        logger.info(f"ğŸ‘‹ {engine_config.symbol} å¼•æ“æ”¶åˆ°ä¸­æ–­ä¿¡å·")
    except Exception as e:
        logger.error(f"âŒ {engine_config.symbol} å¼•æ“è¿›ç¨‹å´©æºƒ: {e}")
        import traceback
        traceback.print_exc()


class MultiProcessArbitrageManager:
    """å¤šè¿›ç¨‹å¥—åˆ©ç®¡ç†å™¨ - ä¸“æ³¨äºå¼•æ“ç®¡ç†å’Œé£æ§æ•°æ®åˆ†å‘"""

    def __init__(self, config: ManagerConfig = None):
        self.config = config or ManagerConfig()
        self.arbitrage_param: Optional[MultiExchangeArbitrageParam] = None
        self.is_running = False
        self.shutdown_event = asyncio.Event()

        # å¤šè¿›ç¨‹ç®¡ç†
        self.process_manager = mp.Manager()
        self.shared_risk_data = self.process_manager.dict()
        self.stop_events: Dict[str, mp.Event] = {}
        self.engine_processes: Dict[str, mp.Process] = {}

        # å¼•æ“é…ç½®å­˜å‚¨
        self.engine_configs: Dict[str, EngineConfig] = {}

        # å¥åº·ç›‘æ§
        self.engine_health: Dict[str, EngineHealthMetrics] = {}

        # é£æ§æ•°æ®ç¼“å­˜
        self.cached_risk_data: Optional[MultiExchangeCombinedInfoModel] = None
        self.last_risk_update_time = 0
        self.last_notify_time = 0

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'start_time': time.time(),
            'total_engines_started': 0,
            'total_engine_restarts': 0
        }

    async def initialize(self):
        """åˆå§‹åŒ–ç®¡ç†å™¨"""
        # åˆå§‹åŒ–äº¤æ˜“æ‰€å‚æ•°
        self.arbitrage_param = MultiExchangeArbitrageParam(auto_init=True)
        await self.arbitrage_param.init_async_exchanges()

        if len(self.arbitrage_param.async_exchanges) < 2:
            raise ValueError("éœ€è¦è‡³å°‘2ä¸ªå¯ç”¨äº¤æ˜“æ‰€æ‰èƒ½è¿è¡Œå¥—åˆ©ç³»ç»Ÿ")
        # åˆå§‹åŒ–é£æ§æ•°æ® - å¿…é¡»åœ¨å¯åŠ¨ä»»ä½•è¿›ç¨‹ä¹‹å‰å®Œæˆ
        await self._initialize_risk_data()

    async def _initialize_risk_data(self):
        """åˆå§‹åŒ–é£æ§æ•°æ®å¹¶åˆ†å‘åˆ°å…±äº«å­—å…¸"""
        try:
            # è·å–é£æ§æ•°æ®
            self.cached_risk_data = await get_multi_exchange_info_combined_model(
                async_exchange_list=self.arbitrage_param.async_exchange_list,
                find_opportunities=False,  # ç®¡ç†å™¨ä¸éœ€è¦å¯»æ‰¾æœºä¼š
                opportunity_limit=0
            )
            self.last_risk_update_time = time.time()

            # ç«‹å³åˆ†å‘ç»™å…±äº«å­—å…¸ï¼Œç¡®ä¿è¿›ç¨‹å¯åŠ¨æ—¶å°±èƒ½è·å–åˆ°
            self.shared_risk_data['risk_data'] = self.cached_risk_data
            self.shared_risk_data['update_time'] = time.time()
            logger.success(f"âœ… é£æ§æ•°æ®:\n {self.cached_risk_data}")

        except Exception as e:
            logger.error(f"âŒ è·å–äº¤æ˜“æ‰€é£æ§æ•°æ®å¤±è´¥: {e}")
            exit()

    async def start_engines_for_whitelist_and_pos(self):
        """ä¸ºç™½åå•ä»£å¸å’ŒæŒä»“ä»£å¸å¯åŠ¨å¼•æ“"""
        if self.cached_risk_data is None:
            raise Exception("é£æ§æ•°æ®æœªåˆå§‹åŒ–ï¼Œæ— æ³•å¯åŠ¨å¼•æ“")
        start_engine_symbol_list = list(set(ArbitrageWhiteListParam.SYMBOL_LIST) | set(self.cached_risk_data.holding_symbol_list))
        # start_engine_symbol_list = ArbitrageWhiteListParam.SYMBOL_LIST
        # é¡ºåºå¯åŠ¨å¼•æ“ï¼Œé¿å…åŒæ—¶å‘èµ·è¿‡å¤šAPIè¯·æ±‚
        for i, symbol in enumerate(start_engine_symbol_list):
            try:
                # é€‰æ‹©æœ€ä½³äº¤æ˜“æ‰€ç»„åˆ
                best_pair = await self._select_optimal_exchange_pair(symbol)
                if not best_pair:
                    logger.warning(f"âŒ {symbol} æœªæ‰¾åˆ°å¯ç”¨äº¤æ˜“æ‰€ç»„åˆ")
                    continue

                # åˆ›å»ºå¼•æ“é…ç½®
                engine_config = EngineConfig(
                    symbol=symbol,
                    exchange1_code=best_pair[0],
                    exchange2_code=best_pair[1],
                    daemon_mode=True
                )

                # å¯åŠ¨å¼•æ“è¿›ç¨‹
                await self._start_engine_process(engine_config)

                # å¯åŠ¨å»¶è¿Ÿï¼Œé¿å…APIè¯·æ±‚è¿‡å¤šï¼ˆæœ€åä¸€ä¸ªä¸éœ€è¦å»¶è¿Ÿï¼‰
                if i < len(ArbitrageWhiteListParam.SYMBOL_LIST) - 1:
                    delay = self.config.engine_startup_delay_sec
                    logger.info(f"â±ï¸  ç­‰å¾… {delay}ç§’...")
                    await asyncio.sleep(delay)

            except Exception as e:
                logger.error(f"âŒ å¯åŠ¨ {symbol} å¼•æ“å¤±è´¥: {e}")
                # å³ä½¿å¤±è´¥ä¹Ÿç»§ç»­å¯åŠ¨ä¸‹ä¸€ä¸ªå¼•æ“
                continue

    async def _select_optimal_exchange_pair(self, symbol: str) -> Optional[tuple]:
        """æ™ºèƒ½é€‰æ‹©æœ€ä¼˜äº¤æ˜“æ‰€ç»„åˆ"""
        available_exchanges = list(self.arbitrage_param.async_exchanges.keys())

        if len(available_exchanges) < 2:
            return None

        logger.info(f"ğŸ” ä¸º {symbol} åˆ†ææœ€ä¼˜äº¤æ˜“æ‰€ç»„åˆï¼Œå€™é€‰äº¤æ˜“æ‰€: {available_exchanges}")

        # è¯„åˆ†ç³»ç»Ÿ
        best_pair = None
        best_score = float('-inf')
        pair_scores = {}

        for i, exc1 in enumerate(available_exchanges):
            for j, exc2 in enumerate(available_exchanges):
                if i >= j:
                    continue  # é¿å…é‡å¤ç»„åˆå’Œè‡ªèº«ç»„åˆ

                # è®¡ç®—ç»¼åˆè¯„åˆ†
                score = await self._calculate_pair_score(exc1, exc2, symbol)
                pair_scores[f"{exc1}-{exc2}"] = score

                if score > best_score:
                    best_score = score
                    best_pair = (exc1, exc2)

        logger.info(f"ğŸ“Š äº¤æ˜“æ‰€ç»„åˆè¯„åˆ†: {pair_scores}")
        logger.success(f"âœ… é€‰æ‹©æœ€ä¼˜ç»„åˆ: {best_pair}, è¯„åˆ†: {best_score:.4f}")

        return best_pair

    async def _calculate_pair_score(self, exc1: str, exc2: str, symbol: str) -> float:
        """è®¡ç®—äº¤æ˜“æ‰€ç»„åˆçš„ç»¼åˆè¯„åˆ†"""
        try:
            # 1. èµ„é‡‘è´¹ç‡å·®å¼‚è¯„åˆ† (æƒé‡ 40%)
            exchange1 = self.arbitrage_param.async_exchanges[exc1]
            exchange2 = self.arbitrage_param.async_exchanges[exc2]

            # ä½¿ç”¨å¼‚æ­¥æ–¹æ³•è·å–èµ„é‡‘è´¹ç‡
            funding_rate1 = await exchange1.get_funding_rate(symbol)
            funding_rate2 = await exchange2.get_funding_rate(symbol)
            funding_rate_diff = abs(funding_rate1 - funding_rate2)
            funding_score = min(funding_rate_diff * 10000, 10.0) * 0.4  # æ ‡å‡†åŒ–åˆ°0-10åˆ†

            # 2. æ‰‹ç»­è´¹è¯„åˆ† (æƒé‡ 20%)
            fee1 = getattr(self.arbitrage_param.async_exchanges[exc1], 'taker_fee_rate', 0.001)
            fee2 = getattr(self.arbitrage_param.async_exchanges[exc2], 'taker_fee_rate', 0.001)
            avg_fee = (fee1 + fee2) / 2
            fee_score = max(0, (0.002 - avg_fee) * 1000) * 0.2  # è´¹ç‡è¶Šä½åˆ†è¶Šé«˜

            # 3. å¯é æ€§è¯„åˆ† (æƒé‡ 25%)
            reliability_scores = {
                'binance': 0.95,
                'hyperliquid': 0.90,
                'lighter': 0.85,
                'aster': 0.80,
                'okx': 0.90,
                'bybit': 0.85
            }
            rel1 = reliability_scores.get(exc1.lower(), 0.70)
            rel2 = reliability_scores.get(exc2.lower(), 0.70)
            reliability_score = (rel1 + rel2) / 2 * 10 * 0.25

            # 4. æµåŠ¨æ€§è¯„åˆ† (æƒé‡ 15%)
            liquidity_score = 0.75  # åŸºç¡€æµåŠ¨æ€§è¯„åˆ†

            total_score = funding_score + fee_score + reliability_score + liquidity_score

            logger.debug(f"ğŸ“ˆ {exc1}-{exc2} è¯„åˆ†è¯¦æƒ…: èµ„é‡‘è´¹ç‡{funding_score:.2f} + æ‰‹ç»­è´¹{fee_score:.2f} + "
                        f"å¯é æ€§{reliability_score:.2f} + æµåŠ¨æ€§{liquidity_score:.2f} = {total_score:.2f}")

            return total_score

        except Exception as e:
            logger.error(f"è®¡ç®—äº¤æ˜“æ‰€ç»„åˆè¯„åˆ†å¤±è´¥ {exc1}-{exc2}: {e}")
            return 0.0

    async def _start_engine_process(self, engine_config: EngineConfig):
        """å¯åŠ¨å•ä¸ªå¼•æ“è¿›ç¨‹"""
        symbol = engine_config.symbol
        process_key = f"{symbol}_{engine_config.exchange1_code}_{engine_config.exchange2_code}"

        if process_key in self.engine_processes and self.engine_processes[process_key].is_alive():
            logger.info(f"âš ï¸  {symbol} å¼•æ“è¿›ç¨‹å·²å­˜åœ¨ï¼Œè·³è¿‡å¯åŠ¨")
            return

        try:
            # åˆ›å»ºåœæ­¢äº‹ä»¶
            stop_event = self.process_manager.Event()
            self.stop_events[process_key] = stop_event

            # åˆ›å»ºå¹¶å¯åŠ¨è¿›ç¨‹
            process = mp.Process(
                target=run_real_engine_in_process,
                args=(engine_config, self.shared_risk_data, stop_event),
                name=f"Engine_{process_key}"
            )

            process.start()
            self.engine_processes[process_key] = process
            self.engine_configs[process_key] = engine_config

            self.stats['total_engines_started'] += 1

            logger.success(f"âœ… {symbol} å¼•æ“è¿›ç¨‹å¯åŠ¨æˆåŠŸ (PID: {process.pid}, {engine_config.exchange1_code}-{engine_config.exchange2_code})")

        except Exception as e:
            logger.error(f"âŒ å¯åŠ¨ {symbol} å¼•æ“è¿›ç¨‹å¤±è´¥: {e}")
            # æ¸…ç†èµ„æº
            if process_key in self.stop_events:
                del self.stop_events[process_key]
            raise

    async def _update_risk_data(self):
        """æ›´æ–°é£æ§æ•°æ®ç¼“å­˜"""
        try:
            self.cached_risk_data = await get_multi_exchange_info_combined_model(
                async_exchange_list=self.arbitrage_param.async_exchange_list,
                find_opportunities=False,  # ç®¡ç†å™¨ä¸éœ€è¦å¯»æ‰¾æœºä¼š
                opportunity_limit=0
            )
            logger.debug(f"ğŸ”„ é£æ§æ•°æ®æ›´æ–°(é—´éš”:{time.time()-self.last_risk_update_time:.0f}s):\n{self.cached_risk_data}")
            self.last_risk_update_time = time.time()

            should, msg = self.cached_risk_data.should_notify_risk()
            if should:
                await async_notify_telegram(f"âŒâŒ {self.arbitrage_param.async_exchanges.keys()}é£æ§æé†’:\n{msg}")
            # åˆ†å‘ç»™æ‰€æœ‰å¼•æ“è¿›ç¨‹
            self.shared_risk_data['risk_data'] = self.cached_risk_data
            self.shared_risk_data['update_time'] = time.time()
            logger.info(f"âœ… é£æ§æ•°æ®:\n{self.cached_risk_data}")

        except Exception as e:
            logger.error(f"âŒ æ›´æ–°é£æ§æ•°æ®å¤±è´¥: {e}")
            exit()


    async def _check_engine_health(self):
        """æ™ºèƒ½æ£€æŸ¥å¼•æ“è¿›ç¨‹å¥åº·çŠ¶æ€"""
        failed_processes = []
        unhealthy_processes = []

        for process_key, process in list(self.engine_processes.items()):
            health_metrics = self.engine_health.get(process_key)

            # åˆå§‹åŒ–å¥åº·æŒ‡æ ‡
            if health_metrics is None:
                self.engine_health[process_key] = EngineHealthMetrics(
                    process_id=process.pid,
                    start_time=time.time()
                )
                health_metrics = self.engine_health[process_key]

            # 1. æ£€æŸ¥è¿›ç¨‹å­˜æ´»æ€§
            if not process.is_alive():
                logger.warning(f"âš ï¸  {process_key} å¼•æ“è¿›ç¨‹å·²åœæ­¢è¿è¡Œ (é€€å‡ºç : {process.exitcode})")
                health_metrics.consecutive_failures += 1
                health_metrics.is_healthy = False
                failed_processes.append(process_key)
                continue

            # 2. æ£€æŸ¥å†…å­˜ä½¿ç”¨
            memory_usage = await self._get_process_memory_usage(process.pid)
            health_metrics.memory_usage_mb = memory_usage

            if memory_usage > self.config.memory_limit_mb:
                logger.warning(f"âš ï¸  {process_key} å†…å­˜ä½¿ç”¨è¿‡é«˜: {memory_usage:.1f}MB > {self.config.memory_limit_mb}MB")
                unhealthy_processes.append(process_key)
                health_metrics.is_healthy = False

            # 3. æ£€æŸ¥è¿è¡Œæ—¶é•¿å’Œé‡å¯æ¬¡æ•°
            if health_metrics.restart_count >= self.config.max_restart_attempts:
                logger.error(f"âŒ {process_key} é‡å¯æ¬¡æ•°å·²è¾¾ä¸Šé™ ({health_metrics.restart_count})")
                unhealthy_processes.append(process_key)
                continue

            # 4. æ£€æŸ¥æ— äº¤æ˜“è¶…æ—¶
            if health_metrics.last_trade_time:
                time_since_trade = time.time() - health_metrics.last_trade_time
                if time_since_trade > self.config.no_trade_timeout_min * 60:
                    logger.warning(f"âš ï¸  {process_key} é•¿æ—¶é—´æ— äº¤æ˜“æ´»åŠ¨: {time_since_trade/60:.1f}åˆ†é’Ÿ")

            # 5. æ›´æ–°å¥åº·çŠ¶æ€
            if process.is_alive() and process_key not in unhealthy_processes:
                health_metrics.is_healthy = True

        # å¤„ç†å¤±è´¥è¿›ç¨‹
        await self._handle_failed_processes(failed_processes)

        # å¤„ç†ä¸å¥åº·è¿›ç¨‹
        await self._handle_unhealthy_processes(unhealthy_processes)

    async def _get_process_memory_usage(self, pid: int) -> float:
        """è·å–è¿›ç¨‹å†…å­˜ä½¿ç”¨é‡(MB)"""
        try:
            import psutil
            process = psutil.Process(pid)
            memory_bytes = process.memory_info().rss
            return memory_bytes / 1024 / 1024
        except ImportError:
            # psutilæœªå®‰è£…ï¼Œè¿”å›ä¼°ç®—å€¼
            return 100.0
        except Exception:
            return 0.0

    async def _handle_failed_processes(self, failed_processes: List[str]):
        """å¤„ç†å¤±è´¥è¿›ç¨‹çš„æ™ºèƒ½é‡å¯"""
        for process_key in failed_processes:
            health_metrics = self.engine_health.get(process_key)
            if not health_metrics:
                continue

            # æ£€æŸ¥æ˜¯å¦åº”è¯¥é‡å¯
            if not self._should_restart_engine(health_metrics):
                logger.error(f"âŒ {process_key} ä¸å†é‡å¯ï¼Œå·²è¾¾åˆ°æœ€å¤§å°è¯•æ¬¡æ•°")
                continue

            try:
                logger.info(f"ğŸ”„ é‡å¯ {process_key} å¼•æ“è¿›ç¨‹ (ç¬¬{health_metrics.restart_count + 1}æ¬¡)...")

                # æ¸…ç†æ—§è¿›ç¨‹èµ„æº
                await self._cleanup_process(process_key)

                # ç­‰å¾…é€€é¿æ—¶é—´
                backoff_time = self.config.restart_backoff_factor ** health_metrics.restart_count
                if backoff_time > 1:
                    logger.info(f"â±ï¸  ç­‰å¾…é€€é¿æ—¶é—´: {backoff_time:.1f}åˆ†é’Ÿ")
                    await asyncio.sleep(backoff_time * 60)

                # é‡æ–°å¯åŠ¨è¿›ç¨‹
                engine_config = self.engine_configs[process_key]
                await self._start_engine_process(engine_config)

                # æ›´æ–°å¥åº·æŒ‡æ ‡
                health_metrics.restart_count += 1
                health_metrics.consecutive_failures = 0
                health_metrics.start_time = time.time()

                self.stats['total_engine_restarts'] += 1
                logger.success(f"âœ… {process_key} å¼•æ“è¿›ç¨‹é‡å¯æˆåŠŸ")

            except Exception as e:
                logger.error(f"âŒ é‡å¯ {process_key} å¼•æ“è¿›ç¨‹å¤±è´¥: {e}")
                health_metrics.consecutive_failures += 1

    async def _handle_unhealthy_processes(self, unhealthy_processes: List[str]):
        """å¤„ç†ä¸å¥åº·çš„è¿›ç¨‹"""
        for process_key in unhealthy_processes:
            health_metrics = self.engine_health.get(process_key)
            if health_metrics:
                health_metrics.is_healthy = False

            # å¯ä»¥é€‰æ‹©é‡å¯ä¸å¥åº·çš„è¿›ç¨‹
            logger.warning(f"âš ï¸  {process_key} è¿›ç¨‹ä¸å¥åº·ï¼Œå°†ç›‘æ§æ˜¯å¦éœ€è¦é‡å¯")

    def _should_restart_engine(self, health_metrics: EngineHealthMetrics) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥é‡å¯å¼•æ“"""
        # æ£€æŸ¥é‡å¯æ¬¡æ•°é™åˆ¶
        if health_metrics.restart_count >= self.config.max_restart_attempts:
            return False

        # æ£€æŸ¥è¿ç»­å¤±è´¥æ¬¡æ•°
        if health_metrics.consecutive_failures > 5:
            return False

        return True

    async def _cleanup_process(self, process_key: str):
        """æ¸…ç†è¿›ç¨‹èµ„æº"""
        try:
            # å‘é€åœæ­¢ä¿¡å·
            if process_key in self.stop_events:
                self.stop_events[process_key].set()
                del self.stop_events[process_key]

            # ç­‰å¾…è¿›ç¨‹é€€å‡º
            if process_key in self.engine_processes:
                process = self.engine_processes[process_key]
                process.join(timeout=10)

                if process.is_alive():
                    logger.warning(f"âš ï¸  {process_key} è¿›ç¨‹æœªæ­£å¸¸é€€å‡ºï¼Œå¼ºåˆ¶ç»ˆæ­¢")
                    process.terminate()
                    process.join(timeout=5)

                del self.engine_processes[process_key]

        except Exception as e:
            logger.error(f"âŒ æ¸…ç† {process_key} è¿›ç¨‹èµ„æºå¤±è´¥: {e}")

    async def _send_status_notification(self):
        """å‘é€å¢å¼ºçŠ¶æ€é€šçŸ¥"""
        if not self.config.enable_notifications:
            return

        current_time = time.time()
        if current_time - self.last_notify_time < self.config.notify_interval_min * 60:
            return

        try:
            active_count = len([p for p in self.engine_processes.values() if p.is_alive()])
            total_started = self.stats['total_engines_started']
            total_restarts = self.stats['total_engine_restarts']

            # è®¡ç®—å¥åº·ç»Ÿè®¡
            healthy_count = len([h for h in self.engine_health.values() if h.is_healthy])
            avg_memory = sum(h.memory_usage_mb for h in self.engine_health.values()) / len(self.engine_health) if self.engine_health else 0
            max_restarts = max((h.restart_count for h in self.engine_health.values()), default=0)

            # è·å–å¼•æ“è¯¦æƒ…
            engine_details = []
            for process_key, health in list(self.engine_health.items())[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                status_emoji = "âœ…" if health.is_healthy else "âŒ"
                memory_str = f"{health.memory_usage_mb:.0f}MB" if health.memory_usage_mb > 0 else "N/A"
                restart_str = f"({health.restart_count}é‡å¯)" if health.restart_count > 0 else ""
                engine_details.append(f"{status_emoji} {process_key} {memory_str} {restart_str}")

            message = (
                f"ğŸ“Š å¥—åˆ©ç®¡ç†å™¨æ™ºèƒ½çŠ¶æ€æŠ¥å‘Š\n"
                f"ğŸ¤– æ´»è·ƒå¼•æ“: {active_count} (å¥åº·: {healthy_count})\n"
                f"ğŸš€ æ€»å¯åŠ¨/é‡å¯: {total_started}/{total_restarts}\n"
                f"ğŸ’¾ å¹³å‡å†…å­˜: {avg_memory:.0f}MB\n"
                f"ğŸ”„ æœ€å¤§é‡å¯æ¬¡æ•°: {max_restarts}\n"
                f"ğŸ• é£æ§æ›´æ–°: {time.strftime('%H:%M:%S', time.localtime(self.last_risk_update_time))}\n"
                f"â±ï¸  è¿è¡Œæ—¶é•¿: {int((time.time() - self.stats.get('start_time', time.time())) / 60)}åˆ†é’Ÿ\n"
            )

            if engine_details:
                message += f"\nğŸ” å¼•æ“è¯¦æƒ…:\n" + "\n".join(engine_details)

            await async_notify_telegram(message, channel_type=CHANNEL_TYPE.QUIET)
            await async_notify_telegram(str(self.cached_risk_data), channel_type=CHANNEL_TYPE.QUIET)
            self.last_notify_time = current_time

        except Exception as e:
            logger.error(f"âŒ å‘é€é€šçŸ¥å¤±è´¥: {e}")

    async def run(self):
        """è¿è¡Œç®¡ç†å™¨ä¸»å¾ªç¯"""
        await self.initialize()
        await self.start_engines_for_whitelist_and_pos()

        self.is_running = True
        self.stats['start_time'] = time.time()

        logger.success("ğŸ¯ å¤šè¿›ç¨‹å¥—åˆ©ç®¡ç†å™¨å¯åŠ¨å®Œæˆ")

        while self.is_running and not self.shutdown_event.is_set():
            try:
                # æ›´æ–°é£æ§æ•°æ®
                await self._update_risk_data()

                # æ£€æŸ¥å¼•æ“å¥åº·çŠ¶æ€
                await self._check_engine_health()

                # å‘é€çŠ¶æ€é€šçŸ¥
                await self._send_status_notification()

                # ç­‰å¾…ä¸‹ä¸€æ¬¡å¾ªç¯ï¼Œä½¿ç”¨çŸ­é—´éš”ä»¥ä¾¿å¿«é€Ÿå“åº”åœæ­¢ä¿¡å·
                wait_interval = self.config.risk_update_interval_min * 60
                # åˆ†è§£é•¿ç­‰å¾…ä¸ºå¤šä¸ªçŸ­ç­‰å¾…ï¼Œç¡®ä¿å¿«é€Ÿå“åº”
                for _ in range(0, wait_interval, 5):  # æ¯5ç§’æ£€æŸ¥ä¸€æ¬¡
                    if self.shutdown_event.is_set():
                        logger.info("ğŸ›‘ ç®¡ç†å™¨æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œé€€å‡ºä¸»å¾ªç¯")
                        break
                    await asyncio.sleep(5)

                if self.shutdown_event.is_set():
                    break  # é€€å‡ºä¸»å¾ªç¯

            except Exception as e:
                logger.error(f"âŒ ç®¡ç†å™¨è¿è¡Œå¼‚å¸¸: {e}")
                # å¼‚å¸¸æ—¶èƒ½å¿«é€Ÿå“åº”åœæ­¢ä¿¡å·
                for _ in range(60):
                    if self.shutdown_event.is_set():
                        logger.info("ğŸ›‘ ç®¡ç†å™¨åœ¨å¼‚å¸¸å¤„ç†ä¸­æ”¶åˆ°åœæ­¢ä¿¡å·")
                        break
                    await asyncio.sleep(1)

    async def shutdown(self):
        """ä¼˜é›…å…³é—­ç®¡ç†å™¨"""
        # å…³é—­å‰å†æ¬¡æ£€æŸ¥ä»“ä½ä¿¡æ¯
        await self._update_risk_data()
        await async_notify_telegram(str(self.cached_risk_data), channel_type=CHANNEL_TYPE.TRADE)
        self.is_running = False
        self.shutdown_event.set()

        # å‘é€åœæ­¢ä¿¡å·ç»™æ‰€æœ‰å¼•æ“è¿›ç¨‹
        for process_key, stop_event in self.stop_events.items():
            try:
                logger.info(f"ğŸ›‘ åœæ­¢ {process_key} å¼•æ“è¿›ç¨‹...")
                stop_event.set()
            except Exception as e:
                logger.error(f"âŒ å‘é€åœæ­¢ä¿¡å·ç»™ {process_key} å¤±è´¥: {e}")

        # ç­‰å¾…æ‰€æœ‰è¿›ç¨‹é€€å‡º - ä½¿ç”¨æ›´æ¿€è¿›çš„è¶…æ—¶ç­–ç•¥
        for process_key, process in self.engine_processes.items():
            try:
                process.join(timeout=3)  # å‡å°‘ç­‰å¾…æ—¶é—´åˆ°3ç§’
                if process.is_alive():
                    logger.warning(f"âš ï¸  {process_key} è¿›ç¨‹æœªåœ¨3ç§’å†…é€€å‡ºï¼Œå¼ºåˆ¶ç»ˆæ­¢")
                    process.terminate()
                    process.join(timeout=2)  # ç»ˆæ­¢åç­‰å¾…2ç§’
                    if process.is_alive():
                        logger.error(f"ğŸš¨ {process_key} è¿›ç¨‹æ— æ³•ç»ˆæ­¢ï¼Œå¼ºåˆ¶æ€æ­»")
                        process.kill()  # ä½¿ç”¨killå¼ºåˆ¶æ€æ­»
                        process.join(timeout=1)
                logger.info(f"âœ… {process_key} å¼•æ“è¿›ç¨‹å·²å…³é—­")
            except Exception as e:
                logger.error(f"âŒ å…³é—­ {process_key} å¼•æ“è¿›ç¨‹å¤±è´¥: {e}")

        # å‘é€æœ€ç»ˆç»Ÿè®¡æŠ¥å‘Š
        await self._send_final_report()

        # æ¸…ç†æ‰€æœ‰èµ„æº
        self.engine_processes.clear()
        self.engine_configs.clear()
        self.engine_health.clear()
        self.stop_events.clear()

        # å…³é—­å¤šè¿›ç¨‹ç®¡ç†å™¨
        try:
            self.process_manager.shutdown()
        except Exception as e:
            logger.error(f"âŒ å…³é—­å¤šè¿›ç¨‹ç®¡ç†å™¨å¤±è´¥: {e}")

    async def _send_final_report(self):
        """å‘é€æœ€ç»ˆè¿è¡ŒæŠ¥å‘Š"""
        if not self.config.enable_notifications:
            return

        try:
            runtime_minutes = int((time.time() - self.stats.get('start_time', time.time())) / 60)

            # ç»Ÿè®¡å¥åº·æ•°æ®
            healthy_engines = len([h for h in self.engine_health.values() if h.is_healthy])
            total_restarts = sum(h.restart_count for h in self.engine_health.values())

            message = (
                f"ğŸ å¥—åˆ©ç®¡ç†å™¨è¿è¡ŒæŠ¥å‘Š\n"
                f"â±ï¸  æ€»è¿è¡Œæ—¶é•¿: {runtime_minutes} åˆ†é’Ÿ\n"
                f"ğŸš€ æ€»å¯åŠ¨æ¬¡æ•°: {self.stats['total_engines_started']}\n"
                f"ğŸ”„ æ€»é‡å¯æ¬¡æ•°: {total_restarts}\n"
                f"ğŸ’š å¥åº·å¼•æ“: {healthy_engines}/{len(self.engine_health)}\n"
                f"ğŸ‘‹ ç®¡ç†å™¨å·²ä¼˜é›…å…³é—­"
            )

            await async_notify_telegram(message)

        except Exception as e:
            logger.error(f"âŒ å‘é€æœ€ç»ˆæŠ¥å‘Šå¤±è´¥: {e}")


# ä¸»ç¨‹åºå…¥å£
async def main():
    """ä¸»ç¨‹åºå…¥å£"""
    # è®¾ç½®å¤šè¿›ç¨‹å¯åŠ¨æ–¹æ³•
    mp.set_start_method('spawn', force=True)

    manager = MultiProcessArbitrageManager()

    # è®¾ç½®ä¿¡å·å¤„ç†
    shutdown_requested = asyncio.Event()
    signal_received = False

    def signal_handler(signum, _):
        nonlocal signal_received
        if signal_received:
            logger.info(f"ğŸ”„ ä¿¡å· {signum} å·²è¢«å¤„ç†ï¼Œå¿½ç•¥é‡å¤ä¿¡å·")
            return

        signal_received = True
        logger.info(f"ğŸ‘‹ æ”¶åˆ°åœæ­¢ä¿¡å· {signum}ï¼Œç«‹å³å¼€å§‹å…³é—­ç¨‹åº...")
        shutdown_requested.set()
        # ç«‹å³è§¦å‘ç®¡ç†å™¨å…³é—­
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                asyncio.create_task(manager.shutdown())
            else:
                # å¦‚æœäº‹ä»¶å¾ªç¯è¿˜æ²¡è¿è¡Œï¼Œè®¾ç½®æ ‡å¿—ä½è®©ä¸»å¾ªç¯æ£€æŸ¥
                logger.info("äº‹ä»¶å¾ªç¯æœªè¿è¡Œï¼Œè®¾ç½®åœæ­¢æ ‡å¿—")
                manager.shutdown_event.set()
        except Exception as e:
            logger.warning(f"è®¾ç½®å…³é—­ä»»åŠ¡æ—¶å‡ºç°å¼‚å¸¸: {e}")
            # è‡³å°‘è®¾ç½®æ ‡å¿—ä½
            manager.shutdown_event.set()

    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)

    try:
        # åˆ›å»ºä»»åŠ¡æ¥è¿è¡Œç®¡ç†å™¨
        manager_task = asyncio.create_task(manager.run())

        # æŒç»­ç­‰å¾…å…³é—­ä¿¡å·
        await shutdown_requested.wait()

        logger.info("ğŸ‘‹ æ”¶åˆ°å…³é—­ä¿¡å·ï¼Œæ­£åœ¨åœæ­¢ç®¡ç†å™¨...")

        # å–æ¶ˆç®¡ç†å™¨ä»»åŠ¡
        manager_task.cancel()

        try:
            await manager_task
        except asyncio.CancelledError:
            pass

        # å°è¯•ä¼˜é›…å…³é—­ï¼Œä½†æœ‰è¶…æ—¶é™åˆ¶
        try:
            await asyncio.wait_for(manager.shutdown(), timeout=10)
        except asyncio.TimeoutError:
            logger.error("âŒ ç®¡ç†å™¨å…³é—­è¶…æ—¶ï¼Œå¼ºåˆ¶é€€å‡º")
            # å¼ºåˆ¶å…³é—­æ‰€æœ‰è¿›ç¨‹
            for process_key, process in manager.engine_processes.items():
                if process.is_alive():
                    logger.warning(f"ğŸš¨ å¼ºåˆ¶æ€æ­»è¿›ç¨‹: {process_key}")
                    process.kill()
            return

    except KeyboardInterrupt:
        logger.info("ğŸ‘‹ æ”¶åˆ°é”®ç›˜ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨å…³é—­...")
        try:
            await asyncio.wait_for(manager.shutdown(), timeout=10)
        except asyncio.TimeoutError:
            logger.error("âŒ é”®ç›˜ä¸­æ–­å…³é—­è¶…æ—¶ï¼Œå¼ºåˆ¶é€€å‡º")
            return
    except Exception as e:
        logger.error(f"âŒ ç¨‹åºå¼‚å¸¸é€€å‡º: {e}")
        await manager.shutdown()


if __name__ == "__main__":
    # é…ç½®æ—¥å¿—
    logger.remove()
    logger.add(
        "logs/simple_arbitrage_manager_{time:YYYY-MM-DD}.log",
        rotation="1 day",
        retention="30 days",
        level="INFO",
        format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {message}"
    )
    logger.add(
        lambda msg: print(msg, end=""),
        level="INFO"
    )

    # è¿è¡Œä¸»ç¨‹åº
    asyncio.run(main())